# Thematic Sentiment Analysis of Reddit Discussions as an Indicator of Wellington Housing Market Trends

**Author:** Gowshik Murugesan  
**Contact:** `gmur474@aucklanduni.ac.nz`  
**Project Date:** May 2025

---

## 1. Project Overview

This repository contains the full data analysis pipeline for the research project, "Thematic Sentiment Analysis of Reddit Discussions as an Indicator of Wellington Housing Market Trends." This study investigates whether real-time, user-generated content on social mediaâ€”specifically the r/Wellington subredditâ€”can provide early insights into housing market shifts, acting as a potential leading indicator.

The methodology employs Natural Language Processing (NLP) techniques, including:
*   **Topic Modelling** using Latent Dirichlet Allocation (LDA) to discover six distinct themes within the housing discourse (e.g., "Housing & Commute," "Job & Housing Search").
*   **Sentiment Analysis** using a validated RoBERTa-based model to quantify public sentiment within these themes.
*   **Temporal and Correlation Analysis** to compare the resulting monthly sentiment time series against official housing market data (sales volume and average price) from Cotality.

The complete analysis is documented in the Jupyter Notebook located at: [`/notebooks/civil788ab_reddit_housing_analysis_may2025.ipynb`](/notebooks/civil788ab_reddit_housing_analysis_may2025.ipynb).

---
## Project Outputs & Resources

### Associated Research Paper
The final research paper submitted for this project can be viewed here:

[**ðŸ“„ CIVIL788AB - Reddit Sentiment & Corelogic - Wellington Housing.pdf**](./report/CIVIL788AB___Reddit_Sentiment___Corelogic___Wellington_Housing.pdf)

### View-Only Executed Notebook
A view-only link to the fully executed Google Colab notebook is available for anyone who wishes to see the code and its outputs without running the analysis themselves.

[**ðŸ’» View on Google Colab**](https://colab.research.google.com/drive/1OJXqPb0EY1gFOxn44P33Bbxo8XCGNSqJ?usp=sharing)

---

## 2. Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vader_validation_sample_MANUALLY_SCORED.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ civil788ab_reddit_housing_analysis_may2025.ipynb
â”œâ”€â”€ report/
â”‚   â””â”€â”€ CIVIL788AB___Reddit_Sentiment___Corelogic___Wellington_Housing.pdf
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ average_sentiment_by_topic.pdf
â”‚   â””â”€â”€ ... (and other generated plots)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
*   **/data:** Contains small, essential data files for the analysis.
*   **/notebooks:** Contains the main Jupyter/Colab notebook with the full analysis pipeline.
*   **/report:** Contains the final research paper in PDF format.
*   **/results:** Contains the key visualisations (PDF charts) generated by the notebook.
*   **README.md:** This documentation file.
*   **requirements.txt:** A list of all Python dependencies required to run the project.

---

## 3. How to Reproduce the Analysis

Follow these steps to set up your environment and run the analysis notebook.

### Step 3.1: Environment Setup

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/mjharavind/wellington-housing-analysis.git
    cd wellington-housing-analysis
    ```

2.  **Create a Python Virtual Environment (Recommended):**
    ```bash
    # For macOS/Linux
    python3 -m venv venv
    source venv/bin/activate

    # For Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

### Step 3.2: Data Acquisition

This project requires two main datasets.

#### Reddit Data (Publicly Available)
The raw Reddit submission data (`Wellington_submissions.zst`) is a large file sourced from **Academic Torrents**, originally collected by Pushshift and community contributors. The notebook is configured to **automatically download** this file for you from a public Google Drive link when you run the data loading cell (Section [3.] in the notebook).

#### Cotality Housing Data (Access Restricted)
The official housing market data is provided by **Cotality** and was accessed under a license via the University of Auckland Library. **Due to these licensing restrictions, this raw data cannot be publicly shared in this repository.**

To reproduce the final correlation analysis (Section [11.] of the notebook), you must have your own licensed access to this data. If you do, please follow these steps:
1.  Export the housing sales data for "Wellington City" from 2018-2024 as a CSV file.
2.  Place this CSV file inside the `/data` folder of this project.
3.  Rename the file to `wellington_housing_data.csv`. The notebook is pre-configured to look for this file name at this location.

If you are a researcher interested in this dataset, please feel free to reach out to me for guidance on potential access through institutional channels.

### Step 3.3: Running the Notebook

The analysis can be run in any environment that supports Jupyter Notebooks (e.g., VS Code, JupyterLab, Kaggle). However, **Google Colab is highly recommended** due to its easy access to powerful GPU hardware, which is essential for timely model training.

1.  **Open in Google Colab:**
    *   Navigate to [colab.research.google.com](https://colab.research.google.com).
    *   Select `File` > `Upload notebook...` and choose the `civil788ab_reddit_housing_analysis_may2025.ipynb` file from the `/notebooks` folder on your computer.

2.  **Upload the `data` Folder:**
    *   In the Colab interface, click the **folder icon** on the left-hand sidebar to open the file browser.
    *   Drag and drop the entire `data` folder (containing `vader_validation_sample_MANUALLY_SCORED.csv`) from your computer into this file browser. This step is essential for the sentiment validation cells to run correctly.

3.  **Set the Runtime to GPU:**
    *   In the Colab menu, navigate to `Runtime` > `Change runtime type`.
    *   Under `Hardware accelerator`, select **T4 GPU**.
    *   **Note:** Access to high-performance GPUs like the T4 is more reliable with a paid plan (e.g., Colab Pro). The RoBERTa and LDA model training cells (Sections [7] and [8]) take ~1-2 minutes on a GPU but may take **over 15-20 minutes** on a standard CPU.

4.  **Run the Cells:**
    *   Execute the notebook cells in order from top to bottom. The notebook is heavily commented to explain each step of the data processing and analysis.

---

## 4. Note on the Manual Validation Step

Section [6.] of the notebook details the validation of the sentiment analysis model. To ensure perfect reproducibility of my findings, the notebook uses the pre-scored file (`vader_validation_sample_MANUALLY_SCORED.csv`) provided in the `/data` folder.

If you wish to perform your own validation, detailed instructions are provided in the notebook for generating a new random sample, which you can then download, score manually, and re-upload to test against the models.
