# Thematic Sentiment Analysis of Reddit Discussions as an Indicator of Wellington Housing Market Trends

**Author:** Gowshik Murugesan  
**Contact:** `gmur474@aucklanduni.ac.nz`  
**Project Start Date:** May 2025

---

## 1. Project Overview

This repository contains the full data analysis pipeline for the research project, "Thematic Sentiment Analysis of Reddit Discussions as an Indicator of Wellington Housing Market Trends." The study investigates whether real-time, user-generated content on social media—specifically the r/Wellington subreddit—can provide early insights into housing market shifts, acting as a potential leading indicator.

The methodology employs Natural Language Processing (NLP) techniques, including:
*   **Topic Modelling** using Latent Dirichlet Allocation (LDA) to discover six distinct themes within the housing discourse (e.g., "Housing & Commute," "Job & Housing Search").
*   **Sentiment Analysis** using a validated RoBERTa-based model to quantify public sentiment within these themes.
*   **Temporal and Correlation Analysis** to compare the resulting monthly sentiment time series against official housing market data (sales volume and average price) from Cotality.

The complete analysis is documented in the Jupyter Notebook located at: `/notebooks/reddit_wellington_housing_analysis.ipynb`.

## 2. Repository Structure

.
├── data/
│ └── vader_validation_sample_MANUALLY_SCORED.csv
├── notebooks/
│ └── reddit_wellington_housing_analysis.ipynb
├── results/
│ ├── average_sentiment_by_topic.pdf
│ ├── roberta_confusion_matrix.pdf
│ ├── ... (and other generated plots)
├── README.md
└── requirements.txt

*   **/data:** Contains small, essential data files. The manually scored sentiment validation sample is located here.
*   **/notebooks:** Contains the main Jupyter/Colab notebook with the full analysis pipeline.
*   **/results:** Contains the key visualisations (PDF charts) generated by the notebook.
*   **README.md:** This documentation file.
*   **requirements.txt:** A list of all Python dependencies required to run the project.

---

## 3. How to Reproduce the Analysis

Follow these steps to set up your environment and run the analysis notebook.

### Step 3.1: Environment Setup

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/[YourGitHubUsername]/wellington-housing-analysis.git
    cd wellington-housing-analysis
    ```

2.  **Create a Python Virtual Environment (Recommended):**
    ```bash
    # For macOS/Linux
    python3 -m venv venv
    source venv/bin/activate

    # For Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

### Step 3.2: Data Acquisition

This project requires two main datasets.

#### Reddit Data (Publicly Available)
The raw Reddit submission data (`Wellington_submissions.zst`) is a large file sourced from **Academic Torrents**, originally collected by Pushshift and community contributors. The notebook is configured to **automatically download** this file for you from a public Google Drive link when you run the data loading cell (Section [3.] in the notebook).

#### Cotality Housing Data (Access Restricted)
The official housing market data is provided by **Cotality** and was accessed under a license via the University of Auckland Library. **Due to these licensing restrictions, this raw data cannot be publicly shared in this repository.**

To reproduce the final correlation analysis (Section [11.] of the notebook), you must have your own licensed access to this data. If you do, please follow these steps:
1.  Export the housing sales data for "Wellington City" from 2018-2024 as a CSV file.
2.  Place this CSV file inside the `/data` folder of this project.
3.  Rename the file to `wellington_housing_data.csv`. The notebook is pre-configured to look for this file name at this location.

If you are a researcher interested in this dataset, please feel free to reach out to me for guidance on potential access through institutional channels.

### Step 3.3: Running the Notebook

The analysis was designed to be run in **Google Colab** to take advantage of free GPU resources, which are highly recommended for the sentiment analysis and topic modelling steps.

1.  **Open in Google Colab:**
    *   Navigate to [colab.research.google.com](https://colab.research.google.com).
    *   Select `File` > `Upload notebook...` and choose the `reddit_wellington_housing_analysis.ipynb` file from the `/notebooks` folder.

2.  **Upload the `data` Folder:**
    *   In the Colab interface, click the **folder icon** on the left-hand sidebar to open the file browser.
    *   Drag and drop the entire `data` folder (containing `vader_validation_sample_MANUALLY_SCORED.csv`) from your computer into this file browser. This step is essential for the sentiment validation cells to run correctly.

3.  **Set the Runtime to GPU:**
    *   In the Colab menu, navigate to `Runtime` > `Change runtime type`.
    *   Under `Hardware accelerator`, select **T4 GPU**.
    *   *Note:* The RoBERTa and LDA model training cells (Sections [7] and [8]) take ~1-2 minutes on a GPU but may take **over 15-20 minutes** on a standard CPU.

4.  **Run the Cells:**
    *   Execute the notebook cells in order from top to bottom. The notebook is heavily commented to explain each step of the data processing and analysis.

---

## 4. Note on the Manual Validation Step

Section [6.] of the notebook details the validation of the sentiment analysis model. To ensure perfect reproducibility of my findings, the notebook uses the pre-scored file (`vader_validation_sample_MANUALLY_SCORED.csv`) provided in the `/data` folder.

If you wish to perform your own validation, detailed instructions are provided in the notebook for generating a new random sample, which you can then download, score manually, and re-upload to test against the models.